\documentclass[a4paper,11pt,reqno]{amsart}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{float}
\usepackage{caption}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{esvect}
\usepackage{scalerel}
\usepackage{gensymb}
\geometry{left=2.5cm, right=2.5cm, bottom=3cm}
% \pagenumbering{roman}

\parskip 1ex
\parindent 0 pt

\newcounter{temp}
\newcounter{prob_counter}
\newcounter{sprob_counter}

\newenvironment{problem}
{\begin{list}{{\bf \arabic{prob_counter}}}{
      \usecounter{prob_counter}
      \addtolength{\labelsep}{.6ex}
      \addtolength{\itemsep}{4.3ex}
      \setlength{\leftmargin}{1.4em}}
      \setcounter{prob_counter}{\value{temp}}
}
{\setcounter{temp}{\value{prob_counter}}
  \end{list}
}

\newenvironment{subprob}
{
  \begin{list}{{\bf \alph{sprob_counter}}}{
      \usecounter{sprob_counter}
      \addtolength{\labelsep}{.6ex}
      \addtolength{\itemsep}{.5ex}
      \setlength{\leftmargin}{1.7em}}
}
{\end{list}}

\newenvironment{solution}{\textbf{Solution.}}{\qed}

\newcommand{\rubrik}[1]{\bigskip \begin{center}{\bf #1}\end{center} \medskip}



\usepackage{xcolor}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
\lstset{language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{keywords},
    commentstyle=\color{comments},
    stringstyle=\color{red},
    showstringspaces=false,
    identifierstyle=\color{green},
    %procnamekeys={def,class}
    numbers=left,
    xleftmargin=2em,
    frame=single,
    framexleftmargin=2.5em
}


\lstset{literate=%
  *{0}{{{\color{green}0}}}1
    {1}{{{\color{green}1}}}1
    {2}{{{\color{green}2}}}1
    {3}{{{\color{green}3}}}1
    {4}{{{\color{green}4}}}1
    {5}{{{\color{green}5}}}1
    {6}{{{\color{green}6}}}1
    {7}{{{\color{green}7}}}1
    {8}{{{\color{green}8}}}1
    {9}{{{\color{green}9}}}1
}


\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}

% \newcommand{\chr}{\chi}
\newcommand{\compl}[1]{\overline{#1}}
\newcommand{\so}{\Rightarrow}
\newcommand{\blocc}{\mathcal{B}}
\newcommand{\val}{\operatorname{val}}
\newcommand{\set}[1]{{#1}}
\newcommand{\sd}{\scaleobj{0.8}{\triangle}}


\DeclareMathOperator*{\Exp}{\mathbb{E}}

\begin{document}

\pagestyle{empty}
\thispagestyle{empty}

{\small{\sc\noindent
        Róbert Leó Jónsson ({\tt robertt20@ru.is}))
}}

\rubrik{Chapter 3}

\section*{Exercise 3.1}
\begin{quote}
Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.  
\end{quote}

\subsection*{Controlling a rocket}
A reinforcement learning agent is to control a rocket that should fly into orbit. 

The agent will want to know all about the forces acting on the rocket, and the current position, velocity, acceleration, heading, and rotation forces of the rocket. It might need to know the fuel level, and the temperature of the engine in order to complete its task.

Finally, most rockets have discrete stages; the first transition might perhaps be to discard the level 1 boosters. 

Thus the state might be composed of
\begin{itemize}
    \item   Position
    \item   Velocity
    \item   Acceleration
    \item   Fuel status
    \item   Engine temperature
    \item   Heading
    \item   Rotation
    \item   The phase of the launch
\end{itemize}

The agent should control the engine and controls of the rocket. Maybe the agent also controls when the rocket releases its boosters. The agent might also want to warn the humans that something is wrong, so it might want to send a distress signal.

Thus the actions could be
\begin{itemize}
    \item   Accelerate engine
    \item   Turn (or something to control the heading of the engine)
    \item   Transition into next phase of launch
    \item   Send distress signal
\end{itemize}

The agent should fly the rocket so that it heads forward, with minimum turbulance, with fuel efficiency, and a nice ascent into orbit. Its reward might be some weighted combination of these factors. 

\subsection*{Programming}

We could face a reinforcement learner create programs for tasks, given a description in some form. In fact, this has already been done. 

The agent might receive as a state the programming language it should use, if it should not decide that itself. Of course it should want a description of the program. 

An important thing to know when programming is the current state of the code, so the agent should receive that as well. 

Now, the agent might want to run the program to see if it produces the desired output. So that might be a possible part of the state. 

For the actions, the agent might choose to input a character to the file, delete text, or any other aciton that modifies the text. The agent might also decide to run to the program to see the results. Finally, when the agent is happy, it should be able to submit its code. 

\subsection*{Cook food}
We might produce a chef agent. It might control some kind of robot that has access to all necessary appliances in a kitchen, and the ingredient, and the agent's goal is to produce the requested dish. 

The states might consist of visual input, temperature of stoves and ovents, and the sensorymotor data from the robot's actuators. 

The agent needs to control the robot, so the actions should include all the possible controls of the robot. Also, for better automation, stove and oven controls might be actions seperate from the robot's movement. 

The rewards should be negative for breaking stuff, or otherwise reaking havoc in the kitchen, and there should be positive rewards for creating the requested dish, where the size of the reward is a funciton of the dish's quality.

\section*{Exercise 3.2}
\begin{quote}
     Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?
\end{quote}

I cannot find any counterexamples.

\section*{Exercise 3.3}

The actions that the agent can perform instantaneously at any time step should be the defined actions for the agent. The choice of where to drive should not be an output for a driving agent, as that is not how you drive a car, in the immediate sense. 

The choice of where to drive should be something that the agent decides makes plans on how to execute, and then uses its actuators/available actions to perform. 

A driving agent that would output a coordinate of where to drive would not be a driving agent, rather a guidance agent. 

Of course, a driving agent could have some other agent that controls where it should drive, and the driving agent implements that plan, but that would be a seperate agent, distinct form the driving agent. 

Now, as stated, the actions should be something that can be outputted, and executed immediately. If the agent outputs a rotational acceleration of the steering wheel, or simply its goal angle should be something that could be immediately implemented by some microcontroller or other machinery. 

Therefore that is an acceptable action. 

Now, if there is some automatic system that can better automate a task than the agent can, say, a good brake system in a car that prevents skidding, the agent should not directly control the force of the brake pads, but should simply gain access to the brake pedal, and let the brake system do the rest.

\section*{Exercise 3.4}

\begin{table}[H]
    \centering
    \caption{The Table}

    \begin{tabular}{l|l|l|r|r}
        \(s\)  & \(a\)  & \(s'\)  & \(r\)  & \(p(s', r \mid s, a )\)  \\
        \hline
         high&  search& high & 1 & 0.4  \\
         high&  search& high & 0 & 0.6 \\
         high&  search& low  & 1 & 0.4 \\
         high&  search& low  & 0 & 0.6  \\
         high&  wait  & high & 1 & 0.1 \\
         high&  wait  & high & 0 & 0.9 \\
         high&  wait  & low  & 1 & 0.1 \\
         high&  wait  & low  & 0 & 0.9  \\
         low &  search& low  & 1 & 0.2 \\
         low &  search& low  & 0 & 0.8 \\
         low &  wait  & low  & 1 & 0 \\
         low &  wait  & low  & 0 & 1 \\
         low &  recharge& low & 1 & 0 \\
         low &  recharge& low & 0 & 1 
    \end{tabular}
\end{table}

\section*{Exercise 3.5}
The formula simply becomes
\[
    p(s', r \mid s, a) = 1, \text{ for all } s \in \mathcal{S} \setminus \mathcal{S}^{+}
\]

\section*{Exercise 3.6}
The return would not just be related to $-\gamma^{K}$, it would equal $-\gamma^{K}$, where $K$ is the number of steps until failure.

In the continuing task formulation, the agent might also take into account the next failure, in the next reset, in a given state. 

\section*{Exercise 3.7}
The expected return is always one, as the agent always escapes the maze eventually, and when it does, there is no discrimination between states. All states get an expected return of one. 

To communicate to the agent that it should navigate the maze efficiently, we should penalize it for every move it makes, so that it wants to escape the maze quickly. We only instructed it to *eventually* escape the maze, no matter how long it takes. 

\section*{Exercise 3.8}
\begin{align}
    &G_5=0\\
    &G_4=R_5 + \gamma G_5 = 2 + 0.5 \cdot 0 = 2\\
    &G_3=R_4 + \gamma G_4 = 3 + 0.5 \cdot 2 = 4\\
    &G_2=R_3+\gamma G_3= 6 + 0.5 \cdot 4 = 8\\
    &G_1=R_2+\gamma G_2=2 + 0.5 \cdot 8 = 6\\
    &G_0=R_1 + \gamma G_1=-1 + 0.5 \cdot 6 = 2
\end{align}

\section*{Exercise 3.9}
We know that 
\begin{align}
    G_1 &=\sum_{i=0}^{\infty} 7\gamma ^{i}=7 \sum_{i =0}^{\infty} \gamma ^{i}\\
        &= 7 \frac{1}{1 - \gamma} = 7 \frac{1}{0.1}\\
        &= 7 \cdot 10 = 70
\end{align}
So 
\begin{align}
    G_0 &=2 + \gamma G_1\\
    &= 2 + 0.9 \cdot 70\\
    &= 2 + 7 = 65
\end{align}

\section*{Exercise 3.10}
Let
\[
    \Gamma = \sum_{k=0}^{\infty} \gamma ^{k}
\]
Then
\begin{align}
    \Gamma - \gamma \Gamma &= \sum_{k =0}^{\infty} \gamma ^{k} - \sum_{k=1}^{\infty} \gamma ^{k}\\
            &= \gamma ^{0}=1
\end{align}

And hence
\begin{align}
    &(1 - \gamma ) \Gamma =1\\
    & \implies \Gamma = \frac{1}{1 - \gamma }
\end{align}

\section*{Exercise 3.11}
If $A$ is the set of actions, and $R$ the set of possible rewards, it is
\[
    \mathbb{E}(R _{t +1}) = \sum_{a \in A} \left[ \pi (a \mid S_t) 
        \sum_{r \in R} \left( r \sum_{s' \in S} p(s',r \mid S_t ,a) \right) 
    \right] 
\]

\section*{Exercise 3.12}
We want to give an equation for $v _{\pi }$ in terms of $q _{\pi }$ and $\pi $. This can be done by taking the action-value function, and averaging out the action, that is
\[
    v _{\pi }(s) = \mathbb{E} \left[ q _{\pi }(s, a) \right] = \sum_{a \in A} \pi (a \mid s) q _{\pi }(s, a)
\]

\section*{Exercise 3.13}
We want to give an equation for $q _{\pi }$ in terms of $v _{\pi }$ and $p(s',r \mid s,a)$. We define $\mathbb{E} _{x} \left[ f(x) \right] $ as the expectation of $f$ with respect to $x$. With that definition, we see that

\begin{align}
    q _{\pi }&= \mathbb{E} _{s', r} \left[ r + \gamma v(s') \right] \\
    &= \sum_{s' \in S} \pi (a \mid s) \sum_{r \in R} p(s', r \mid s, a)(r + \gamma v(s'))
\end{align}

\section*{Exercise 3.14}

We represent the state $s$ as its coordinate $s = (x, y)$. 

Thus,
\begin{align}
    v _{\pi }(2, 2) &= 0.7\\
    v _{\pi }(2, 1) &= 2.5\\
    v _{\pi} (1, 2) &= 0.7 \\
    v _{\pi }(3,2) &= 0.4\\
    v _{\pi }(2, 3) &= -0.4
\end{align}

Now, the Bellman equation for the value function $v _{\pi }$ is
\[
    v _{\pi }(s) = \sum_{a} \pi (a \mid a) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v _{\pi }(s') \right] 
\]

We how that the Bellman equation for the value function holds for $v _{\pi }(2, 2)$ with respect to its neighbors. The set of actions is $A=(UP,  RIGHT, DOWN, LEFT)$. However, all teh actions from the state $s=(2, 2)$ result in a reward of $0$. 

Also, the environment is deterministic. We can now perform the calculation:

\begin{align}
    v _{\pi }(2, 2) &= \sum_{a} \pi(a \mid  s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma  v _{\pi }(s') \right] \\
    &= \frac{1}{4}(\gamma v _{\pi }(2, 1) + \gamma v _{\pi }(1, 2) + \gamma v _{\pi }(1, 2) + \gamma v _{\pi }(3, 2) + \gamma  v _{\pi } (2, 3))\\
    &= \frac{0.9}{4}(2.5 + 0.7 + 0.4 + (-0.4))\\
    &= 0.72 \approx 0.7
\end{align}

\section*{Exercise 3.15}
If we add a constant $c$ to all the rewards, each reward becomes $R_t + c$, so the return becomes
\begin{align}
    G'_t &= (R_{t + 1} + c) + (R_{t + 2} + c) + \cdots \\
        &= \sum_{k=0}^{\infty} \gamma ^{k}(R_{t + k + 1}  + c) \\
        &= \sum_{k=0}^{\infty} \gamma ^{k}c + \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1} \\
        &= \frac{c}{1 - \gamma} + \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1}   \\
        &= G_{t} + \frac{c}{1 - \gamma } 
\end{align}
So the value $v'_\pi $ becomes 
\begin{align}
    v'_\pi (s) &= \mathbb{E}\left[ G'_t  \mid S_t = s \right] \\
&= \mathbb{E}\left[ G_t + \frac{c}{1 - \gamma } \mid S_t =s \right]  \\
&= \mathbb{E}\left[ G_t \mid S_t = s \right] + \mathbb{E}\left[ \frac{c}{1 - \gamma } \mid S_t = s \right]   \\
&= \mathbb{E}\left[ G_t \mid S_t = s \right] + \frac{c}{1 - \gamma }\\
&= v_{\pi }(s) + \frac{c}{1 - \gamma } \\
&= v_{\pi }(s) + v_c 
\end{align}
where the constant added is $v_c =\frac{c}{ 1-  \gamma }$

\section*{Exercise 3.16}
Lets say the task is episodic, and some episode lasts $k$ steps. Then, at time $t$, the return is

\begin{align}
    G'_t &= \sum_{i=0}^{k - t - 1} \gamma^{i} (R_{t + i + 1} + c)\\
    &= \sum_{i=0}^{k - t - 1} \gamma ^{i}c  + \sum_{i=0}^{k - t - 1} R_{t + i + 1} \\
    &= \frac{c(\gamma ^{k - t - 1} - 1)}{\gamma  - 1} + \sum_{i=0}^{k - t- 1} R_{t + i + 1} \\
\end{align}
So here we see that the return depends on the number of steps remaining until the end of the episode. 

The feature of the continuing tasks that caused there to be a constant added to the value (or return), is that the sum was infinite. 

\section*{Exercise 3.17}
We derive the Bellman equation for the action-value function:
\begin{align}
q(s, a) &= \mathbb{E}_\pi \left[ G_t \mid S_t =s, A_t = a \right]  \\
&= E_\pi [R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \\
&= \sum_{s'} \sum_{r} p(s', r\mid s, a)(r + \gamma \mathbb{E}_\pi \left[ G_{t+1} \mid S_{t+1} = s' \right]  \\
&= \sum_{s', r}p(s', r \mid s, a)(r + \gamma \sum_{a'}\pi(a' \mid s') \mathbb{E}\left[ G_{t+1}\mid s', a' \right])   \\
&= \sum_{s', r}p(s', r \mid s, a)\Big(r + \gamma \sum_{a'} \pi (a'\mid s) q(a', s')\Big)  \\
\end{align}

\section*{Exercise 3.18}
This is simply 
\begin{align}
    v_\pi(s) &= \mathbb{E}_\pi \left[ q_{\pi }(a, s) \mid S_t = s \right] \\
        &= \sum_{a} \pi (a\mid s) q_{\pi }(a,s) \\
\end{align}

\section*{Exercise 3.19}
This is
\begin{align}
    q_\pi (s, a) &= \mathbb{E}\left[ R_{t+1} +\gamma  v_{\pi }(S_{t+1}) \mid S_t = s, A_t = a \right] \\
    &= \sum_{s', r}p(s', r \mid s, a)(r + \gamma v_{\pi }(s'))  \\
\end{align}

\section*{Exercise 3.20}
It is the same as the figure of $q_{*}(s, driver)$, but the whole green area has value $-1$. 


\section*{Exercise 3.21}
The same as $v_{putt}$. 

\section*{Exercise 3.22}
We consider the top state to be the only state, as otherwise the definition of the optimal policy would not work. This is because the optimal policy must satisfy $\pi _{*}(s) \ge \pi (s)$ for all other policies $\pi $, which would not be the case if this was the given MDP. 

f $\gamma =0$, then $\pi _{left}$ is the optimal policy, as the return in the top state is then $+1$. If $\gamma =0.9$, however, then the values of the top state for the two policies are
\begin{align}
    v_{\pi _{right}}(s) &= \sum_{i=0}^{\infty} 0.9^{2i + 1} \cdot 2\\
            &=\sum_{i=0}^{\infty} 0.81^{i} \cdot 0.9 \cdot 2 \\
            &= 1.8 \sum_{i=1}^{\infty} 0.81^{i} \\
            &= 1.8(\frac{1}{1 - 0.81} - 0.81)  = 8.19 \\
    v_{\pi _{left}}(s) &= \sum_{i=0}^{\infty} 0.9^{2i} \cdot 1\\
        &= \sum_{i=0}^{\infty} 0.81^{i} \\
        &= \frac{1}{1 - 0.81} = 5.26 \\
\end{align}
So in that case $\pi _{right}$ is better. For $\gamma =0.5$, we have
\begin{align}
    v_{\pi _{right}}(s) &= \sum_{i=0}^{\infty} 0.5^{2i + 1} \cdot 2\\
            &=\sum_{i=0}^{\infty} 0.25^{i} \cdot 0.5 \cdot 2 \\
            &= \sum_{i=1}^{\infty} 0.25^{i} \\
            &= \frac{1}{1 - 0.25 } - 0.25= 1.08 \\
    v_{\pi _{left}}(s) &= \sum_{i=0}^{\infty} 0.5^{2i}\\
        &= \sum_{i=0}^{\infty} 0.25^{i} \\
        &= \frac{1}{1 - 0.25} = 1.333 \\
\end{align}
so $\pi _{left}$ is better.


\section*{Exercise 3.23}
TODO

\section*{Exercise 3.24}
We first need to find the value of $\gamma $ used. Notice that since 
\begin{align}
G_t = R_{t + 1} + \gamma G_{t + 1}\\
\implies \gamma  = \frac{G_t - R_{t + 1}}{G_{t + 1}}
\end{align}
we can use two given values to find $\gamma $. We use the state $A$. Then $G_t = 24.4$, $R_{t + 1} = 10$, and $G_{t + 1} = 16$. Thus
\begin{align}
\gamma  = \frac{24.4 - 10}{16} = 0.9
\end{align}
The following python program will find $v_{*}$ to any given accuracy:
\begin{lstlisting}[language=python]
import numpy as np

UP=1
DOWN=2
LEFT=3
RIGHT=4

GAMMA = 0.9

pistar = [
    [RIGHT, DOWN, LEFT, UP, LEFT], 
    [UP, UP, UP, LEFT, LEFT], 
    [UP for _ in range(5)],
    [UP for _ in range(5)],
    [UP for _ in range(5)]
]

def action(x, y, dir):
    if x == 0 and y == 1:
        return 4, 1, 10

    if x == 0 and y == 3:
        return 2, 3, 5

    if dir == UP:
        x -= 1
    elif dir == DOWN:
        x += 1
    elif dir == LEFT:
        y -= 1
    elif dir == RIGHT:
        y += 1
    return x, y, 0

def get_valuestar(x, y):
    cur_gamma = 1
    cur_value = 0
    for i in range(100):
        x, y, r = action(x, y, pistar[x][y])
        cur_value += cur_gamma * r
        cur_gamma *= GAMMA
    return cur_value
        
values = np.zeros((5, 5))

for i in range(5):
    for j in range(5):
        values[i][j] = get_valuestar(i, j)

print(values)

\end{lstlisting}
And we obtained
\begin{table}[htpb]
    \centering
    \caption{Gridworld values}
    \label{tab:label}

\begin{tabular}{|rrrrr|}
\hline
21.977 & 24.419 & 21.977 & 19.419 & 17.477 \\
19.780 & 21.977 & 19.780 & 17.802 & 16.022 \\
17.802 & 19.780 & 17.802 & 16.022 & 14.419 \\
16.022 & 17.802 & 16.022 & 14.419 & 12.977 \\
14.419 & 16.022 & 14.419 & 12.977 & 11.680 \\
\hline
        
    \end{tabular}
\end{table}


\section*{Exercise 3.25}
We of course have
 \begin{align}
v_{*}(s) = \max_{a \in \mathcal{A}(s)}q_{*}(s, a) \label{eq:vstarq}
\end{align}

\section*{Exercise 3.26}
Since we have \eqref{eq:vstarq}, we have
\begin{align}
q_{*}(s, a) = \sum_{s', r} p(s', r  \mid s, a) \left[ r + \gamma v_{*}(s') \right] 
\end{align}

\section*{Exercise 3.27}
This is simply
\begin{align}
\pi _{*}(s) = \operatorname{argmax}_{a \in \mathcal{A}(s)} q_{*}(s, a)
\end{align}

\section*{Exercise 3.28}
This is 
\begin{align}
\pi _{*}(s) = \operatorname{argmax}_{a \in \mathcal{A}(s)A} \sum_{s', r} p(s', r  \mid s, a) \left[ r + \gamma v_{*}(s') \right]
\end{align}

\section*{Exercise 3.29}
In terms of $p(s' \mid s, a)$ and $r(s, a)$, we have
\begin{align}
    &v_{\pi }(s) = \sum_{a} \pi (a \mid s) \left( r(s, a) + \gamma  \sum_{s'}  
    p(s' \mid s, a) v_{\pi }(s')
\right) \\
    &v_{* }(s) = \max_{a} \left( r(s, a) + \gamma  \sum_{s'}  
    p(s' \mid s, a) v_{*}(s')
\right) \\
    &q_{\pi }(s, a) = r(s, a) + \gamma \left( \sum_{s'}p(s' \mid s, a) \sum_{a'}\pi (a' \mid s') q_{*}(s', a')   \right) \\
   &q_{*}(s, a) = r(s, a) + \gamma \left( \max_{a'} \sum_{s'}p(s' \mid s, a')q_{*}(s', a')  \right) 
\end{align}



































\end{document}


































