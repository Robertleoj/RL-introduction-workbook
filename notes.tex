\documentclass[a4paper,11pt,reqno]{amsart}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{float}
\usepackage{caption}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{esvect}
\usepackage{scalerel}
\usepackage{csquotes}
\usepackage{gensymb}
\usepackage{mdframed}
\geometry{left=2.5cm, right=2.5cm, bottom=3cm}
% \pagenumbering{roman}

\parskip 1ex
\parindent 0 pt

\newcounter{temp}
\newcounter{prob_counter}
\newcounter{sprob_counter}

\newenvironment{problem}
{\begin{list}{{\bf \arabic{prob_counter}}}{
      \usecounter{prob_counter}
      \addtolength{\labelsep}{.6ex}
      \addtolength{\itemsep}{4.3ex}
      \setlength{\leftmargin}{1.4em}}
      \setcounter{prob_counter}{\value{temp}}
}
{\setcounter{temp}{\value{prob_counter}}
  \end{list}
}

\newenvironment{subprob}
{
  \begin{list}{{\bf \alph{sprob_counter}}}{
      \usecounter{sprob_counter}
      \addtolength{\labelsep}{.6ex}
      \addtolength{\itemsep}{.5ex}
      \setlength{\leftmargin}{1.7em}}
}
{\end{list}}

\newenvironment{solution}{\textbf{Solution.}}{\qed}

\newcommand{\rubrik}[1]{\bigskip \begin{center}{\bf #1}\end{center} \medskip}

\newmdtheoremenv{theorem}{Theorem}
\newmdtheoremenv{corrolary}{Corrolary}
\newmdtheoremenv{definition}{Definition}
\newmdtheoremenv{remark}{Remark}
   

\usepackage{xcolor}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
\lstset{language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{keywords},
    commentstyle=\color{comments},
    stringstyle=\color{red},
    showstringspaces=false,
    identifierstyle=\color{green},
    %procnamekeys={def,class}
    numbers=left,
    xleftmargin=2em,
    frame=single,
    framexleftmargin=2.5em
}


\lstset{literate=%
  *{0}{{{\color{green}0}}}1
    {1}{{{\color{green}1}}}1
    {2}{{{\color{green}2}}}1
    {3}{{{\color{green}3}}}1
    {4}{{{\color{green}4}}}1
    {5}{{{\color{green}5}}}1
    {6}{{{\color{green}6}}}1
    {7}{{{\color{green}7}}}1
    {8}{{{\color{green}8}}}1
    {9}{{{\color{green}9}}}1
}

\newcommand{\vect}[1]{\boldsymbol{#1}}

\begin{document}

\pagestyle{empty}
\thispagestyle{empty}

{\small{\sc\noindent
        Róbert Leó Jónsson ({\tt robertt20@ru.is}))
}}

\rubrik{RL notes}

\section*{Markov Decision Processes (MDP)}
The at time $t$ the state is $S_t \in \mathcal{S}$, the action is $A_t \in \mathcal{A}$, and the reward (received before seeing the state and doing the action) is $R_t \in \mathcal{R}$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, and $\mathcal{R} \subset \mathbb{R}$ is the reward space.

The \textbf{trajectory} is 
\begin{align}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
\end{align}
The probability of getting to state $s'$ and gettign reward $r$ after taking action $a$ in state $s$ is well defined, and given by
\begin{align}
p(s', r \mid s, a) = \operatorname{Pr} \left\{ S_t = s', R_t = r \mid S_{t - 1}, A_{t - 1} = a \right\} 
\end{align}
where the function $p$ defines \textbf{dynamics} of the MDP, and is called the \textbf{dynamics function}. The probabilities given $p$ completely characterize the environment's dynamics. This also confirms that an MDP has the Markov property. 

We can obtain the \textbf{state-transition probabilities} with
\begin{align}
p(s' \mid s, a) = \operatorname{Pr} (S_t = s' \mid S_{t - 1} = s, A_{t - 1} = a) = \sum_{r \in \mathcal{R}} p(s', r \mid s, a)
\end{align}
And the expected rewards for a state-action pair:
\begin{align}
r(s, a) = \mathbb{E}\left[ R_t \mid S_{t - 1} = s, A_{t - 1} =a\right]  = \sum_{r \in \mathcal{R}}r \sum_{s' \in \mathcal{S}} p(s', r \mid s, a)
\end{align}
and the expected reward given the next state:
\begin{align}
r(s, a, s') = \mathbb{E}\left[ R_t \mid S_{t - 1} = s, A_{t -1 } = a, S_t = s' \right] = \sum_{r \in \mathcal{R}} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}
\end{align}

\section*{Returns and episodes}
The \textbf{return} is the discounted sum of rewards (if we are using discounting).

In an episodic task, this is
\begin{align}
G_t = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2} R_{t + 3} + \ldots + \gamma^{T  - t - 1} R_T
\end{align}
where $T$ is the final time step. We think of each episode ending in the \textbf{same} terminal state - can be though of as an artificial state that occurs right \emph{after} the real terminal state of the episode. 

The final reward is given in this final terminal state. 

For continuing tasks, the return is
\begin{align}
G_t = \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1}
\end{align}
If we define the reward to be zero after the final state, this also holds for episodic tasks.

Theree is a recursive relationship between $G_t$ and $G_{t + 1}$: 
\begin{align}
    G_t &= \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1}\\
&= R_t + \sum_{k=1}^{\infty} \gamma ^{k}R_{t + k + 1} \\
&= R_t + \gamma \sum_{k=1}^{\infty} \gamma ^{k - 1}R_{t + k + 1} \\
&= R_t + \gamma \sum_{k=0}^{\infty}\gamma ^{k} R_{t + k + 2} \\
&= R_t + \gamma G_{t + 1} \\
\end{align}

\section*{Policies and value function}
The \textbf{value} function is defined to be
\begin{align}
    v_{\pi }(s) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]\\  
    &= \mathbb{E}\left[ \sum_{k=0}^{\infty} \gamma ^{k}R_{t+k+1} \;\Bigg | \;  S_t + s \right] 
\end{align}
and the \textbf{action-value} function is
\begin{align}
q_\pi (s, a) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]  \\
&= \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1} \;\Bigg | \; S_t = s, A_t = a \right]  \\
\end{align}

The \textbf{Bellman equation} for the value function is
\begin{align}
v_\pi (s) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]  \\
&= \mathbb{E}_\pi \left[ R_{t + 1} + \gamma G_{t + 1} \mid S_t = s \right]  \\
&= \sum_{a} \pi (a \mid s) \sum_{s'} \sum_{r} p(s', r \mid s, a) \left[ r + \gamma \mathbb{E}_\pi \left[ G_{t + 1} \mid S_{t + 1} = s' \right]  \right]  \\
&= \sum_{a} \pi (a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_\pi (s') \right]  \\
\end{align}

And the Bellman equation for the action-value function is
\begin{align}
q(s, a) &= \mathbb{E}_\pi \left[ G_t \mid S_t =s, A_t = a \right]  \\
&= E_\pi [R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \\
&= \sum_{s'} \sum_{r} p(s', r\mid s, a)(r + \gamma \mathbb{E}_\pi \left[ G_{t+1} \mid S_{t+1} = s' \right]  \\
&= \sum_{s', r}p(s', r \mid s, a)(r + \gamma \sum_{a'}\pi(a' \mid s') \mathbb{E}\left[ G_{t+1}\mid s', a' \right])   \\
&= \sum_{s', r}p(s', r \mid s, a)\Big(r + \gamma \sum_{a'} \pi (a'\mid s) q(a', s')\Big)  \\
\end{align}

We can write $v_\pi$ in terms of $q_\pi $ :
\begin{align}
    v_\pi(s) &= \mathbb{E}_\pi \left[ q_{\pi }(a, s) \mid S_t = s \right] \\
        &= \sum_{a} \pi (a\mid s) q_{\pi }(a,s) \\
\end{align}

Or we could write $q_\pi $ in terms of $v_\pi$:
\begin{align}
    q_\pi(s, a) &= \mathbb{E}\left[ R_{t+1} + \gamma v_\pi (S_{t+1}) \mid S_t = s, A_t = a \right] \\
        &= \sum_{s', r'} p(s', r \mid s, a)(r + \gamma v_\pi (s')) \\
\end{align}


\section*{Optimal policies and optimal value function}
The \textbf{optimal policies} are denoted $\pi _*$, and they define the optimal value function
\begin{align}
    v_{*}(s) = \max_\pi v_\pi (s) = v_{\pi_* }(s), \text{ for all }s \in \mathcal{S}
\end{align}
They also define the optimal action-value function
\begin{align}
    q_{*}(s, a) = \max_\pi q_\pi (s, a) = q_{\pi_* }(s, a),
\end{align}
for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$.

$q_*(s, a)$ follows an optimal policy after the action $a$, so we have
\begin{align}
q_{*}(s, a) = \mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \mid S_{t}=s, A_{t} = a \right] 
\end{align}

Now, the \textbf{Bellman optimality equation} for $v_{*}$ is
\begin{align}
    v_{*}(s) &= \max_{a \in \mathcal{A}(s)}q_{\pi _{*}}(s, a) \\
    &= \max_{a}\mathbb{E}_{\pi _{*}}\left[ G_t \mid S_t = s, A_t = a \right]  \\
    &= \max_{a} \mathbb{E}_{\pi _{*}}\left[ R_{t + 1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right]  \\
    &= \max_{a}\mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \mid S_t = s, A_t = a \right]  \\
    &= \max_{a}\sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{*}(s') \right]  \\
\end{align}

The corresponding equation for $q_{*}$ is 
\begin{align}
    q_{*}(s, a) &= \mathbb{E}\left[ R_{t+1} + \gamma \max_{a'}q_{*}(S_{t+1}, a') \;\Big | \; S_t = s, A_t = a \right] \\
    &= \sum_{s', r} p(s', r \mid s, a)\left[ r + \gamma \max_{a'}q_{*}(s', a') \right]  
\end{align}



\end{document}
