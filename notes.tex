\documentclass[a4paper,11pt,reqno]{amsart}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{float}
\usepackage{caption}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{esvect}
\usepackage{scalerel}
\usepackage{csquotes}
\usepackage{gensymb}
\usepackage{mdframed}
\geometry{left=2.5cm, right=2.5cm, bottom=3cm}
% \pagenumbering{roman}

\parskip 1ex
\parindent 0 pt

\newcounter{temp}
\newcounter{prob_counter}
\newcounter{sprob_counter}

\newenvironment{problem}
{\begin{list}{{\bf \arabic{prob_counter}}}{
      \usecounter{prob_counter}
      \addtolength{\labelsep}{.6ex}
      \addtolength{\itemsep}{4.3ex}
      \setlength{\leftmargin}{1.4em}}
      \setcounter{prob_counter}{\value{temp}}
}
{\setcounter{temp}{\value{prob_counter}}
  \end{list}
}

\newenvironment{subprob}
{
  \begin{list}{{\bf \alph{sprob_counter}}}{
      \usecounter{sprob_counter}
      \addtolength{\labelsep}{.6ex}
      \addtolength{\itemsep}{.5ex}
      \setlength{\leftmargin}{1.7em}}
}
{\end{list}}

\newenvironment{solution}{\textbf{Solution.}}{\qed}

\newcommand{\rubrik}[1]{\bigskip \begin{center}{\bf #1}\end{center} \medskip}

\newmdtheoremenv{theorem}{Theorem}
\newmdtheoremenv{corrolary}{Corrolary}
\newmdtheoremenv{definition}{Definition}
\newmdtheoremenv{remark}{Remark}
   

\usepackage{xcolor}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
\lstset{language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{keywords},
    commentstyle=\color{comments},
    stringstyle=\color{red},
    showstringspaces=false,
    identifierstyle=\color{green},
    %procnamekeys={def,class}
    numbers=left,
    xleftmargin=2em,
    frame=single,
    framexleftmargin=2.5em
}


\lstset{literate=%
  *{0}{{{\color{green}0}}}1
    {1}{{{\color{green}1}}}1
    {2}{{{\color{green}2}}}1
    {3}{{{\color{green}3}}}1
    {4}{{{\color{green}4}}}1
    {5}{{{\color{green}5}}}1
    {6}{{{\color{green}6}}}1
    {7}{{{\color{green}7}}}1
    {8}{{{\color{green}8}}}1
    {9}{{{\color{green}9}}}1
}

\newcommand{\vect}[1]{\boldsymbol{#1}}

\begin{document}

\pagestyle{empty}
\thispagestyle{empty}

{\small{\sc\noindent
        Róbert Leó Jónsson ({\tt robertt20@ru.is}))
}}

\rubrik{RL notes}

\section*{Markov Decision Processes (MDP)}
The at time $t$ the state is $S_t \in \mathcal{S}$, the action is $A_t \in \mathcal{A}$, and the reward (received before seeing the state and doing the action) is $R_t \in \mathcal{R}$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, and $\mathcal{R} \subset \mathbb{R}$ is the reward space.

The \textbf{trajectory} is 
\begin{align}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
\end{align}
The probability of getting to state $s'$ and gettign reward $r$ after taking action $a$ in state $s$ is well defined, and given by
\begin{align}
p(s', r \mid s, a) = \operatorname{Pr} \left\{ S_t = s', R_t = r \mid S_{t - 1}, A_{t - 1} = a \right\} 
\end{align}
where the function $p$ defines \textbf{dynamics} of the MDP, and is called the \textbf{dynamics function}. The probabilities given $p$ completely characterize the environment's dynamics. This also confirms that an MDP has the Markov property. 

We can obtain the \textbf{state-transition probabilities} with
\begin{align}
p(s' \mid s, a) = \operatorname{Pr} (S_t = s' \mid S_{t - 1} = s, A_{t - 1} = a) = \sum_{r \in \mathcal{R}} p(s', r \mid s, a)
\end{align}
And the expected rewards for a state-action pair:
\begin{align}
r(s, a) = \mathbb{E}\left[ R_t \mid S_{t - 1} = s, A_{t - 1} =a\right]  = \sum_{r \in \mathcal{R}}r \sum_{s' \in \mathcal{S}} p(s', r \mid s, a)
\end{align}
and the expected reward given the next state:
\begin{align}
r(s, a, s') = \mathbb{E}\left[ R_t \mid S_{t - 1} = s, A_{t -1 } = a, S_t = s' \right] = \sum_{r \in \mathcal{R}} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}
\end{align}

\section*{Returns and episodes}
The \textbf{return} is the discounted sum of rewards (if we are using discounting).

In an episodic task, this is
\begin{align}
G_t = R_{t + 1} + \gamma R_{t + 2} + \gamma^{2} R_{t + 3} + \ldots + \gamma^{T  - t - 1} R_T
\end{align}
where $T$ is the final time step. We think of each episode ending in the \textbf{same} terminal state - can be though of as an artificial state that occurs right \emph{after} the real terminal state of the episode. 

The final reward is given in this final terminal state. 

For continuing tasks, the return is
\begin{align}
G_t = \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1}
\end{align}
If we define the reward to be zero after the final state, this also holds for episodic tasks.

Theree is a recursive relationship between $G_t$ and $G_{t + 1}$: 
\begin{align}
    G_t &= \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1}\\
&= R_t + \sum_{k=1}^{\infty} \gamma ^{k}R_{t + k + 1} \\
&= R_t + \gamma \sum_{k=1}^{\infty} \gamma ^{k - 1}R_{t + k + 1} \\
&= R_t + \gamma \sum_{k=0}^{\infty}\gamma ^{k} R_{t + k + 2} \\
&= R_t + \gamma G_{t + 1} \\
\end{align}

\section*{Policies and value function}
The \textbf{value} function is defined to be
\begin{align}
    v_{\pi }(s) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]\\  
    &= \mathbb{E}\left[ \sum_{k=0}^{\infty} \gamma ^{k}R_{t+k+1} \;\Bigg | \;  S_t + s \right] 
\end{align}
and the \textbf{action-value} function is
\begin{align}
q_\pi (s, a) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]  \\
&= \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma ^{k}R_{t + k + 1} \;\Bigg | \; S_t = s, A_t = a \right]  \\
\end{align}

The \textbf{Bellman equation} for the value function is
\begin{align}
v_\pi (s) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]  \\
&= \mathbb{E}_\pi \left[ R_{t + 1} + \gamma G_{t + 1} \mid S_t = s \right]  \\
&= \sum_{a} \pi (a \mid s) \sum_{s'} \sum_{r} p(s', r \mid s, a) \left[ r + \gamma \mathbb{E}_\pi \left[ G_{t + 1} \mid S_{t + 1} = s' \right]  \right]  \\
&= \sum_{a} \pi (a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_\pi (s') \right]  \\
\end{align}

And the Bellman equation for the action-value function is
\begin{align}
q_{\pi }(s, a) &= \mathbb{E}_\pi \left[ G_t \mid S_t =s, A_t = a \right]  \\
&= E_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right]  \\
&= \sum_{s'} \sum_{r} p(s', r\mid s, a)\left( r + \gamma \mathbb{E}_\pi \left[ G_{t+1} \mid S_{t+1} = s' \right] \right)   \\
&= \sum_{s', r}p(s', r \mid s, a)\left( r + \gamma \sum_{a'}\pi(a' \mid s') \mathbb{E}\left[ G_{t+1}\mid s', a' \right] \right)    \\
&= \sum_{s', r}p(s', r \mid s, a)\left( r + \gamma \sum_{a'} \pi (a'\mid s) q_{\pi }(a', s') \right)  \\
\end{align}

We can write $v_\pi$ in terms of $q_\pi $ :
\begin{align}
    v_\pi(s) &= \mathbb{E}_\pi \left[ q_{\pi }(a, s) \mid S_t = s \right] \\
        &= \sum_{a} \pi (a\mid s) q_{\pi }(a,s) \\
\end{align}

Or we could write $q_\pi $ in terms of $v_\pi$:
\begin{align}
    q_\pi(s, a) &= \mathbb{E}\left[ R_{t+1} + \gamma v_\pi (S_{t+1}) \mid S_t = s, A_t = a \right] \\
        &= \sum_{s', r'} p(s', r \mid s, a)(r + \gamma v_\pi (s')) \\
\end{align}


\section*{Optimal policies and optimal value function}
The \textbf{optimal policies} are denoted $\pi _*$, and they define the optimal value function
\begin{align}
    v_{*}(s) = \max_\pi v_\pi (s) = v_{\pi_* }(s), \text{ for all }s \in \mathcal{S}
\end{align}
They also define the optimal action-value function
\begin{align}
    q_{*}(s, a) = \max_\pi q_\pi (s, a) = q_{\pi_* }(s, a),
\end{align}
for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$.

$q_*(s, a)$ follows an optimal policy after the action $a$, so we have
\begin{align}
q_{*}(s, a) = \mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \mid S_{t}=s, A_{t} = a \right] 
\end{align}

Now, the \textbf{Bellman optimality equation} for $v_{*}$ is
\begin{align}
    v_{*}(s) &= \max_{a \in \mathcal{A}(s)}q_{\pi _{*}}(s, a) \\
    &= \max_{a}\mathbb{E}_{\pi _{*}}\left[ G_t \mid S_t = s, A_t = a \right]  \\
    &= \max_{a} \mathbb{E}_{\pi _{*}}\left[ R_{t + 1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right]  \\
    &= \max_{a}\mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \mid S_t = s, A_t = a \right]  \\
    &= \max_{a}\sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{*}(s') \right]  \\
\end{align}

The corresponding equation for $q_{*}$ is 
\begin{align}
    q_{*}(s, a) &= \mathbb{E}\left[ R_{t+1} + \gamma \max_{a'}q_{*}(S_{t+1}, a') \;\Big | \; S_t = s, A_t = a \right] \\
    &= \sum_{s', r} p(s', r \mid s, a)\left[ r + \gamma \max_{a'}q_{*}(s', a') \right]  
\end{align}


\section*{Policy evaluation}
If the environment's dynamics are completely known, we can approximate $v_{\pi}$ by starting with an arbitrary value function $v_0$ (but with the value of the terminal state equal to zero), and continuously perform the following iteration:
\begin{align}
v_{k+1} &= \mathbb{E}_{\pi }\left[ R_{t+1} + \gamma v_{k}(S_{t+1}) \mid S_t = s \right]  \\
&= \sum_{a} \pi (a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{k}(s') \right]
\end{align}
for all $s \in \mathcal{S}$. In this case,  $v_{k} \rightarrow v_{\pi }$ as $k \rightarrow \infty$.
This is called \textbf{iterative policy evaluation}. 

The analogous iteration for $q_{\pi }(s, a)$ is 
\begin{align}
q_{k + 1}(s, a) &= \mathbb{E}_{\pi }\left[ R_{t + 1} + \gamma G_{t + 1} \mid S_{t} = s, A_{t} = a \right]  \\
&= \sum_{s', r} p(s', r \mid s, a) \left( r + \gamma \sum_{a'} \pi(a' \mid s) q_{k}(a', s)  \right)  \\
\end{align}

\section*{Policy Improvement}
If we have a deterministic policy $\pi $ and its value function $v_{\pi }$, then suppose $\pi '$ is such that in state $s$, we choose the next action $a$ greedily with respect to $v_{\pi }$, then the value of this behavior is
\begin{align}
q_{\pi }(s, a) &= \mathbb{E}\left[ R_{t+1} + \gamma v_{\pi }(S_{t+1} \mid S_{t}=s, A_t = a \right]  \\
&= \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi }(s') \right]  \\
\end{align}
Then $\pi '$ is better than $\pi $ overall.

The \textbf{policy improvement theorem} states that if $\pi $ and $\pi '$ is a pair of deterministic policies such that
\begin{align}
    \forall s \in \mathcal{S}: q_{\pi }(s, \pi '(s)) \ge v_{\pi }(s) \label{qineq}
\end{align}
Then $\pi '$ is as good or better than $\pi $, that is
\begin{align}
    \forall s \in \mathcal{S}: v_{\pi '}(s) \ge v_{\pi }(s) \label{vineq}
\end{align}
And if there is a strict inequality in \eqref{qineq} in $s$, then there is also a strict inequality in \eqref{vineq}, in $s$. 

Now, if we have a policy $\pi $, and $q_{\pi }(s, a)$, then we can construct the new \emph{greedy} policy
\begin{align}
\pi '(s) &= \operatorname{argmax}_{a}q_{\pi }(s, a) \\
&= \operatorname{argmax}_{a}\mathbb{E}\left[ R_{t+1} + \gamma v_{\pi }(S_{t+1} \mid S_{t}=s, A_t =a \right]  \\
&= \operatorname{argmax}_{a}\sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi }(s') \right]  \\
\end{align}
This policy satisfies \eqref{qineq}, so it is as good as or better than $\pi i$. This process is called \emph{policy improvement}. If $\pi '$ is not better than $\pi $ but exactly as good, then we have
\begin{align}
v_{\pi '}(s) &= \max_a \mathbb{E}\left[ R_{t+1} + \gamma v_{\pi '}(S_{t+1}) \mid S_{t}=s, A_t = a \right] \\
&= \max_a \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi '}(s') \right]  \\
\end{align}
which is the Bellman optimality equation, so $v_{\pi '}=v_{*}$, and $\pi $ and $\pi '$ are optimal policies. 

For stochastic policies, this also works - as long as wee assign a zero probability to non-optimal actions, all probability assignments are allowed. 

\section*{Policy Iteration}
\textbf{Policy iteration} is the process of taking a policy, calculating its value function, improving the policy, calculating the value function, and so on:
\begin{align}
\pi _0 \xrightarrow[]{E} v_{\pi _0} \xrightarrow[]{I} \pi _1 \xrightarrow[]{E} v_{\pi _1} \xrightarrow[]{I} \cdots  \xrightarrow[]{I}\pi _* \xrightarrow[]{E}v_{*}
\end{align}


\section*{Value Iteration}
The \textbf{value iteration} algorithm can be written as a simple update operation that combines the policy improvement and truncated policy evaluation steps:
\begin{align}
v_{k+1}(s) &= \max_a \mathbb{E}\left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_{t}=s, A_{t}=a \right]  \\
&= \max_a \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_k(s') \right]  \\
\end{align}
for all $s \in  \mathcal{S}$. 

\section*{Off-policy prediction via Importance Sampling}
Given a starting state $S_t$, the probability of the subsequent state-action trajectory $A_t, S_{t+1}, A_{t+1}, \ldots, S_T$ occurring under any policy $\pi $ is
\begin{align}
&P(A, S_{t+1}, A_{t+1}, \ldots, S_T \mid S_t, A_{t:T_1} \thicksim \pi )\\
&= \pi (A_t \mid S_t) p(S_{t+1} \mid S_t, A_t)\pi (S_{t+1} \mid S_{t+1}) \cdots p(S_T \mid S_{T-1}, A_{T-1}) \\
&= \prod_{k=t}^{T-1} \pi (A_k \mid S_k) p(S_{k+1} \mid S_k, A_k)  
\end{align}
This gives the importance-sampling ratio:
\begin{align}
    \rho _{t:T-1} &= \frac{
    \prod_{k=t}^{T-1}\pi (A_k \mid S_k) p(S_{k+1}\mid S_k, A_k)
}{
    \prod_{k=t}^{T-1} b(A_k \mid S_k) p(S_{k+1}\mid S_k, A_k) 
}\\
    &= \prod_{k=t}^{T-1} \frac{\pi (A_k \mid S_k)}{b(A_k \mid S_k)}
\end{align}
Thus, if we want to approximate $v_{\pi }$, but we only have episodes from the policy $b$, we can fix the expected value of the return with
\begin{align}
\mathbb{E}\left[ \rho _{t:T-1}G_t \mid S_t = s \right] =v_{\pi }(s)
\end{align}
To make the estimate, we can use either \textbf{ordinary importance sampling}, with
\begin{align}
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho _{t:T(t) - 1}G_t}{|\mathcal{T}(s)|}
\end{align}
where $\mathcal{T}(s)$ is the set of time points where $S_{t} = s$ (only the first state in each episode if we are using first-visit sampling), and $T(t)$ is the end time of the episode. 

We can also use \textbf{weighted importance sampling}, with
\begin{align}
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho _{t:T(t) - 1}G_t}{\sum_{t \in \mathcal{T}(s)} \rho _{t:T(t) - 1}}
\end{align}

For action-values, this becomes
\begin{align}
Q(s, a) = \frac{\sum_{t \in \mathcal{T}(s)} \rho _{t+1:T(t) - 1}G_t}{\sum_{t \in \mathcal{T}(s)} \rho _{t + 1:T(t) - 1}}
\end{align}
or 
\begin{align}
Q(s, a) = \frac{\sum_{t \in \mathcal{T}(s)} \rho _{t+1:T(t) - 1}G_t}{|\mathcal{T}(s)|}
\end{align}
for the ordinary importance sampling case. 












\end{document}


































